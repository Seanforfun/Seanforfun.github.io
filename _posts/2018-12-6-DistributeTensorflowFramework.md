---
layout: post
title:  "Tensorflow Distributed Framework"
date:   2018-12-06 12:07
author: Botao Xiao
categories: Tensorflow
comment: true
description: 集群式的tensorflow是从单机多卡演化出来的，当硬件限制了单机上的GPU数量以及内存容量。此时我们需要进行集群式的训练，换句话说就是分布式训练。在极度无聊于深度学习设计网络和充当民工训练的过程中，我深感于人生正在被极度的浪费。人生还是要苦中作乐一些，于是我发现了Tensorflow的分布式模块，想要设计出一套可以易于填充的集群处理框架。由于我个人的水平问题，并没有在Python中发现类似于Spring中IOC和AOP的功能，所以只能用我自己写的一些丑陋的代码去实现类似的功能。
---
集群式的tensorflow是从单机多卡演化出来的，当硬件限制了单机上的GPU数量以及内存容量。此时我们需要进行集群式的训练，换句话说就是分布式训练。在极度无聊于深度学习设计网络和充当民工训练的过程中，我深感于人生正在被极度的浪费。人生还是要苦中作乐一些，于是我发现了Tensorflow的分布式模块，想要设计出一套可以易于填充的集群处理框架。由于我个人的水平问题，并没有在Python中发现类似于Spring中IOC和AOP的功能，所以只能用我自己写的一些丑陋的代码去实现类似的功能。

### 基础原理
1. 分布式的基础原理：反向传播（通过链式法则实现）：
![Imgur](https://i.imgur.com/hlgDBmn.png)
    * W5为第五层layer中存储的参数，我们通过W5进行正向传播，也就是我们需要优化的参数。
    * n为先学习率。
    * W5+为更新后的参数，用于下一次的正向传播。
    * ∂E/∂w(gradient)就是当前的loss对于当前层的参数的的导数，就是我们所说的梯度。
2. 通过研究链式法则，当前层的参数是已知的，需要求出更新后的参数， 其中learning rate是我们设置的，唯一需要使用的参数就是梯度。所以把握了梯度就是把握了网络更新的钥匙。
3. 当我们向网络传入不同的训练集时，对于不同的数据，我们得到的loss是不同的，我们要维持网络中的所有参数都是相同的，这就要保证对于每一层的更新梯度是相同的，所以此处我们要通过求均值的方法获得平均的梯度。这就是多节点的核心思想。

### 单机多GPU训练
![Imgur](https://i.imgur.com/nsK6I0a.png)
1. 所有的网络参数都存在CPU上，CPU作为Server存在，可以发现GPU模型并从GPU模型中获取梯度信息，通过梯度信息更新CPU中的模型。
2. 每个GPU都是一个Tower模型，在GPU中会进行正向传播并算出loss并求出gradiants，Tower模型会返回Gradiant给统一更新的CPU。
3. 上述方法保证了每个GPU中使用的模式是一致的，这是同步更新，由于是单机操作，很小的开销被用于数据的交互。
具体的代码实现可以参照[GMAN深度去雾项目](https://github.com/Seanforfun/GMAN_Net_Haze_Removal)。

